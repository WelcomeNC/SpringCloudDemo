input {
    jdbc {
      # 连接的数据库地址和哪一个数据库，指定编码格式，禁用SSL协议，设定自动重连
      jdbc_connection_string => "jdbc:mysql://192.168.31.54:3306/cs1_db?characterEncoding=UTF-8&useSSL=false&autoReconnect=true"
      # 你的账户密码
      jdbc_user => "root"
      jdbc_password => "123456"
      # 连接数据库的驱动包，建议使用绝对地址
      jdbc_driver_library => "/usr/local/elk-6.5.4/mysql-connector-java-5.1.46/mysql-connector-java-5.1.46-bin.jar"
      # 这是不用动就好
      jdbc_driver_class => "com.mysql.jdbc.Driver"
      jdbc_paging_enabled => "true"
      jdbc_page_size => "10000"
      # 是否将 column 名称转小写
      lowercase_column_names => false
      #使用其它字段追踪，而不是用时间
      use_column_value => "true"
      #追踪的字段
      tracking_column => "id"
      record_last_run => "false"
      #上一个sql_last_value值的存放文件路径, 必须要在文件中指定字段的初始值
      last_run_metadata_path => "/usr/local/elk-6.5.4/logstash-6.5.4/bin/order_detail_parameter.txt"
      #是否清除 last_run_metadata_path 的记录,如果为真那么每次都相当于从头开始查询所有的数据库记录
      clean_run => "true"
      jdbc_default_timezone => "Asia/Shanghai"
      statement_filepath => "/usr/local/elk-6.5.4/logstash-6.5.4/config/order_detail.sql"
      # 这是控制定时的，重复执行导入任务的时间间隔，第一位是分钟
      schedule => "*/1 * * * *"
      type => "order_detail"
    } 

}
filter {
	
}

output {
    stdout{
   codec => json_lines
 }
   if [type]=="order_detail" {
    elasticsearch {
        user => admin
        password => admin
        ssl => true
        ssl_certificate_verification => false
        cacert => "/usr/local/elk-6.5.4/elasticsearch-6.5.4/config/spock.pem"
      	# 要导入到的Elasticsearch所在的主机
        hosts => ["192.168.71.246:9200"]
        #protocol => "http"
        # 要导入到的Elasticsearch的索引的名称
        index => "order_detail"
        # 类型名称（类似数据库表名）
        document_type => "order_detail"

        document_id => "%{id}"
    	} 
   }
 }
